#!/usr/bin/env python

import sys, os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import datetime
from subprocess import call
import urllib
import json
import numpy as np
import cv2
from opensfm import dataset
from opensfm import multiview
from opensfm import reconstruction
from opensfm import transformations as tf


def run_dataset(run_root, name):
    folder = run_root + '/' + name
    call(['cp', 'config.yaml', folder])
    call(['bin/run_all', folder])

    with open(run_root + '/index.html', 'a') as fout:
        s = '''
<a href="/viewer/reconstruction.html#file=../{0}/reconstruction.json">{1}</a><br>
'''.format(urllib.quote(folder), name)
        fout.write(s)

def align_reconstructions(rgt, r):
    if len(r['shots']) < 3: return
    # Compute similarity Xp = s A X + b
    X, Xp = [], []
    for key in rgt['shots']:
        if key in r['shots']:
            X.append(reconstruction.optical_center(r['shots'][key]))
            Xp.append(reconstruction.optical_center(rgt['shots'][key]))
        else:
            print 'Shot {0} missing.'.format(key)
    X = np.array(X)
    Xp = np.array(Xp)
    T = tf.superimposition_matrix(X.T, Xp.T, scale=True)

    A, b = T[:3,:3], T[:3,3]
    s = np.linalg.det(A)**(1./3)
    A /= s
    reconstruction.apply_similarity(r, s, A, b)

def optical_center_rms(rgt, r):
    X = []
    Xp = []
    for key in r['shots']:
        X.append(reconstruction.optical_center(r['shots'][key]))
        Xp.append(reconstruction.optical_center(rgt['shots'][key]))
    X = np.array(X)
    Xp = np.array(Xp)
    return np.sqrt(((X - Xp)**2).sum() / len(X))

def focal_length_rms(rgt, r):
    f = []
    fp = []
    for key in r['shots']:
        cam = r['shots'][key]['camera']
        f.append(r['cameras'][cam]['focal'])
        fp.append(rgt['cameras'][cam]['focal'])
    f = np.array(f)
    fp = np.array(fp)
    return np.sqrt(((f - fp)**2).sum() / len(f))
   


def eval_reconstruction(name, rgt, r):
    align_reconstructions(rgt, r)
    f = focal_length_rms(rgt, r)
    o = optical_center_rms(rgt, r)
    print '#####################################################################'
    print '# Evaluation for', name
    print '#   Focal lenght RMS error:', f
    print '#   Optical center RMS error:', o
    print '#####################################################################'
    return {
        "focal_length_rms": f,
        "optical_center_rms": o,
    }


def load_strecha_gt_projections(run_root, name):
    source = 'eval/datasets/{0}/urd'.format(name)
    r = {
        'cameras': {},
        'shots': {},
    }
    for i in os.listdir(source):
        if i.endswith('.png'):
            P = np.loadtxt(source + '/' + i + '.P')
            K, R, t = multiview.KRt_from_P(P)
            r['cameras']['canon eos d60'] = {
                'focal': K[0,0]
            }
            r['shots'][i] = {
                'camera': 'canon eos d60',
                'rotation': list(cv2.Rodrigues(R)[0].flat),
                'translation': list(t.flat)
            }
    return r    

def eval_strecha_dataset(run_root, name):
    exif = {
        "camera": "canon eos d60",
        "focal_prior": 0.881057268722467, 
    }
    folder = run_root + '/' + name
    source = '../../../datasets/{0}/urd'.format(name)
    call(['mkdir', '-p', folder])
    call(['ln', '-s',  source, folder + '/images'])
    with open(folder + '/exif_overrides.json', 'w') as fout:
        d = { '%04d.png'%i: exif for i in range(30)}
        json.dump(d, fout, indent=4)

    run_dataset(run_root, name)

    with open(folder+'/reconstruction.json') as fin:
        r = json.load(fin)[0]
    rgt = load_strecha_gt_projections(run_root, name)
    return eval_reconstruction(name, rgt, r)


if __name__ == '__main__':
    if len(sys.argv) == 2:
        run_root = sys.argv[1]
    else:
        run_root = 'eval/runs/' + datetime.datetime.now().isoformat()

    # Create evaluation folders
    call(['mkdir', '-p', run_root])

    # Bundler examples
    # for i in ['ET', 'kermit']:
    #     a = run_root + '/' + i
    #     call(['mkdir', '-p', a])
    #     call(['ln', '-s',  '../../../datasets/bundler-v0.4-source/examples/' + i, a + '/images'])

    # Strecha datasets
    results = {}
    for name in [
                 'fountain_dense',
                 'herzjesu_dense',
                 'castle_entry_dense',
                 'castle_dense',
                 'herzjesu_dense_large',
                 'castle_dense_large',
                 ]:
        results[name] = eval_strecha_dataset(run_root, name)

    with open(run_root + '/results.json', 'w') as fout:
        json.dump(results, fout, indent=4)

